{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c4bc666-34a1-47b6-8018-1ad79dafae95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: rgb(230, 216, 173); padding: 10px; height: 70px; border-radius: 15px;\">\n",
    "    <div style=\"font-family: 'Georgia', serif; font-size: 20px; padding: 10px; text-align: right; position: absolute; right: 20px; color: #0a0a0a;\">\n",
    "        <span style=\"color: Black;\">Neel Adalja</span> <br>\n",
    "        <span style=\"font-family: 'Arial', sans-serif; font-size: 12px; color: #0a0a0a;\">Data Scientist | Data Analyst</span>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ad3d8-0bce-4e29-86da-5ea91b5a69df",
   "metadata": {},
   "source": [
    "<div style=\"background-color:hsl(184, 33.00%, 56.10%); padding: 10px; text-align: center; color: black; font-size: 32px; font-family: 'Arial', sans-serif;\">\n",
    "    Face Recognition System <br>\n",
    "    <h3 style=\"text-align: center; color: black; font-size: 15px; font-family: 'Arial', sans-serif;\">Python | OpenCV </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ec8637",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightgreen; color: black; padding: 4px;\">\n",
    "    <h3> PROJECT DESCRIPTION \n",
    "</h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "0fb1cb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\binay\\anaconda3\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\binay\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\binay\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\binay\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\binay\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\binay\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install tensorflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "515efbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries in the notebook\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications import MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "a695a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam using OpenCV\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "811ea883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames\n",
    "\n",
    "ret, frame = cap.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bf589f",
   "metadata": {},
   "source": [
    "Save the captured images for labeled classes:\n",
    "- Display the live feed.\n",
    "- Use key events (`q`,`s`) to quit or save the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "5b5e1d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 's' to save the image, 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "# code for capturing and saving images\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create directories for each person (label)\n",
    "label = \"Binayak Lamichhanec\"  # Change this for different classes\n",
    "output_dir = f\"./data/{label}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"Press 's' to save the image, 'q' to quit.\")\n",
    "\n",
    "image_count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('s'):  # Save the frame\n",
    "        image_path = os.path.join(output_dir, f\"{label}_{image_count}.jpg\")\n",
    "        cv2.imwrite(image_path, frame)\n",
    "        image_count += 1\n",
    "        print(f\"Saved: {image_path}\")\n",
    "    \n",
    "    elif key == ord('q'):  # Quit the capture\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed41c1",
   "metadata": {},
   "source": [
    "**2. Detect and Crop Faces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "26359f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory where images are saved\n",
    "image_dir = \"./data/Binayak Lamichhanec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "7fd950e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using image: ./data/Binayak Lamichhanec\\Binayak Lamichhanec_3.jpg\n"
     ]
    }
   ],
   "source": [
    "# Get the latest saved image from the directory\n",
    "images = sorted(os.listdir(image_dir), key=lambda x: os.path.getctime(os.path.join(image_dir, x)))\n",
    "if len(images) == 0:\n",
    "    print(\"No images found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "latest_image_path = os.path.join(image_dir, images[-1])\n",
    "print(f\"Using image: {latest_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "9959ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the latest image\n",
    "image = cv2.imread(latest_image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d563dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade face detection model using OpenCV.\n",
    "# This model will be used to detect faces in the captured images.\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "c0108ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "0cb8f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face detection with adjusted parameters\n",
    "faces = face_cascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.05,  # Reduce the scale step for finer detection\n",
    "    minNeighbors=6,    # Increase for stricter face detection\n",
    "    minSize=(50, 50)   # Minimum face size to detect\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "98e8cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bounding boxes around detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14f22f",
   "metadata": {},
   "source": [
    "`cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)` \n",
    "- faces is a list of tuples where each tuple represents a detected face.\n",
    "- (x, y) - Coordinates of the top-left corner of the bounding box.\n",
    "- w and h - Width and Height of the bounding box.\n",
    "- (255, 0, 0),2: The color of the rectangle (in BGR format, this is blue) with 2 thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337e45be",
   "metadata": {},
   "source": [
    "Example: `faces = [(50, 50, 100, 100), (200, 150, 80, 80)]`\n",
    "\n",
    "For the first face:\n",
    "- (x, y) = (50, 50)\n",
    "- (x + w, y + h) = (150, 150)\n",
    "\n",
    "For the second face:\n",
    "- (x, y) = (200, 150)\n",
    "- (x + w, y + h) = (280, 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "3cdbb9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image with bounding boxes\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ae9913f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Draw bounding boxes and add labels\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw rectangle with custom color and thickness\n",
    "        color = (0, 255, 0)  # Green color for the box\n",
    "        thickness = 2\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, thickness)\n",
    "        \n",
    "        # Add label above the rectangle\n",
    "        label = \"Face\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.5\n",
    "        font_thickness = 1\n",
    "        label_size, _ = cv2.getTextSize(label, font, font_scale, font_thickness)\n",
    "        \n",
    "        label_x = x\n",
    "        label_y = y - 10 if y - 10 > 10 else y + 10  # Adjust position if label goes out of bounds\n",
    "        cv2.rectangle(image, (label_x, label_y - label_size[1] - 2), (label_x + label_size[0], label_y + 2), color, cv2.FILLED)\n",
    "        cv2.putText(image, label, (label_x, label_y), font, font_scale, (0, 0, 0), font_thickness)\n",
    "\n",
    "# Display the image with bounding boxes and labels\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f655a7",
   "metadata": {},
   "source": [
    "**Preprocess and Crop Faces**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d15ca",
   "metadata": {},
   "source": [
    "- Load saved images\n",
    "- Detect faces in each image using Haar Cascade\n",
    "- Crop the face regions for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "8ec2dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed face: ./processed_faces/face_0.jpg\n"
     ]
    }
   ],
   "source": [
    "output_dir = './processed_faces/'  # Directory to save preprocessed faces\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if len(faces) == 0:\n",
    "    print(\"No faces detected.\")\n",
    "else:\n",
    "    # Loop through detected faces\n",
    "    for idx, (x, y, w, h) in enumerate(faces):\n",
    "        # Crop the face\n",
    "        cropped_face = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Resize to a uniform size (e.g., 160x160 for models like FaceNet)\n",
    "        resized_face = cv2.resize(cropped_face, (160, 160))\n",
    "        \n",
    "        # Save the preprocessed face\n",
    "        face_path = os.path.join(output_dir, f\"face_{idx}.jpg\")\n",
    "        cv2.imwrite(face_path, resized_face)\n",
    "        print(f\"Saved preprocessed face: {face_path}\")\n",
    "        \n",
    "        # Display the preprocessed face (optional)\n",
    "        cv2.imshow(\"Preprocessed Face\", resized_face)\n",
    "        cv2.waitKey(0) # The 0 argument tells OpenCV to wait indefinitely for the user to press any key\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94004bbf",
   "metadata": {},
   "source": [
    "**3. Generate Face Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b8e1e4",
   "metadata": {},
   "source": [
    "Generating face embeddings is about converting a face image into a numerical representation (a vector of numbers) that *captures its unique features*.\n",
    "\n",
    "- Input: You give the model a preprocessed face image (e.g., cropped and resized).\n",
    "\n",
    "- Model Processing: The model analyzes the facial features, like the shape of the eyes, nose, and mouth.\n",
    "\n",
    "- Output (Embedding): The model outputs a set of numbers (embedding) that uniquely represents the face, making it easy to compare with other faces for tasks like recognition or clustering.\n",
    "\n",
    "Think of it as turning a face into a \"digital fingerprint\" for machines to understand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5deef8ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[360], line 10\u001b[0m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m MobileNetV2(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      6\u001b[0m                     include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m      7\u001b[0m                     input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# If you have downloaded a model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m    190\u001b[0m         filepath,\n\u001b[0;32m    191\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[0;32m    193\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\saving\\legacy_h5_format.py:116\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    114\u001b[0m opened_new_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath, h5py\u001b[38;5;241m.\u001b[39mFile)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opened_new_file:\n\u001b[1;32m--> 116\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(filepath, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     f \u001b[38;5;241m=\u001b[39m filepath\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[38;5;241m=\u001b[39mswmr)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\h5py\\_hl\\files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, flags, fapl\u001b[38;5;241m=\u001b[39mfapl)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = 'model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "model = MobileNetV2(weights='imagenet', \n",
    "                    include_top=False, \n",
    "                    input_shape=(160, 160, 3))\n",
    "\n",
    "# If you have downloaded a model\n",
    "model = load_model('model.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff04dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for embeddings and labels\n",
    "embeddings = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bcd9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the folder for preprocessed faces\n",
    "processed_faces_dir = './processed_faces'\n",
    "\n",
    "# Generate embeddings for all preprocessed faces\n",
    "\n",
    "# Iterate through all images in the directory\n",
    "for image_name in os.listdir(processed_faces_dir):\n",
    "    image_path = os.path.join(processed_faces_dir, image_name)\n",
    "\n",
    "    # Check if the file is a valid image (optional: you can adjust based on file types)\n",
    "    if image_name.endswith('.jpg') or image_name.endswith('.jpeg') or image_name.endswith('.png'):\n",
    "        # Load and preprocess image\n",
    "        face_image = cv2.imread(image_path)\n",
    "        face_image = cv2.resize(face_image, (160, 160))  # Ensure correct input size\n",
    "        face_preprocessed = face_image.astype('float32') / 255.0  # Normalize\n",
    "        \n",
    "        # Generate embedding\n",
    "        embedding = model.predict(np.expand_dims(face_preprocessed, axis=0))[0]\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Use the image filename as the label (without file extension)\n",
    "        label = os.path.splitext(image_name)[0]\n",
    "        labels.append(label)\n",
    "\n",
    "print(f\"Generated embeddings for {len(embeddings)} faces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45de025",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b78d6b",
   "metadata": {},
   "source": [
    "**4. Train a Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baa124",
   "metadata": {},
   "source": [
    "- Collect embeddings and corresponding labels.\n",
    "- Train a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(embeddings, labels)\n",
    "print(\"Classifier trained.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6236695",
   "metadata": {},
   "source": [
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e228c3c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[309], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test on a new face\u001b[39;00m\n\u001b[0;32m      2\u001b[0m new_face \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./processed_faces/test_face.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m new_embedding \u001b[38;5;241m=\u001b[39m get_embedding(facenet_model, new_face)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Predict the identity\u001b[39;00m\n\u001b[0;32m      6\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m knn\u001b[38;5;241m.\u001b[39mpredict([new_embedding])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "# Test on a new face\n",
    "new_face = cv2.imread('./processed_faces/test_face.jpg')\n",
    "new_embedding = get_embedding(facenet_model, new_face)\n",
    "\n",
    "# Predict the identity\n",
    "predicted_label = knn.predict([new_embedding])\n",
    "print(\"Predicted Identity:\", predicted_label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51531ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0eb1bf25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93324ac3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13e86481-eae2-4019-9515-66a43a30f0fb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #002147; color: #fff; padding: 30px; text-align: center;\">\n",
    "    <h1>THANK YOU!\n",
    "        <!--- Ram | Tech Skills Trainer | AI/ML Consultant --->\n",
    "</h1> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92ba4c-672c-4e9f-b842-2b2d9234e5ff",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color: rgb(230, 216, 173); color: #2f4f4f; padding: 10px; border-radius: 10px; width: 350px; text-align: center; float: right; margin: 20px 0;\">\n",
    "    Neel Adalja<br>\n",
    "    <span style=\"font-size: 12px; color:rgb(0, 0, 0);\">\n",
    "        Data Analyst | Data Scientist\n",
    "    </span>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc27b3-58d0-431e-8121-f1b4c08377c7",
   "metadata": {},
   "source": [
    "<!--- Mohammad Idrees Bhat | Tech Skills Trainer | AI/ML Consultant --->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
